{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thie file contains the code to reading data in Spark for parallel processing\n",
    "\n",
    "1) importing the dependencies <br>\n",
    "driver memory is set to allocate more memory to spark application <br>\n",
    "when running in standalone mode, memory has to be provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Path for spark source folder\n",
    "os.environ['SPARK_HOME']=\"/home/ashutosh/Dropbox/spark-2.1.1-bin-hadoop2.7\"\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = ' --driver-memory 50g pyspark-shell'\n",
    "\n",
    "# Append pyspark  to Python Path\n",
    "sys.path.append(\"/home/ashutosh/Dropbox/spark-2.1.1-bin-hadoop2.7/python\")\n",
    "sys.path.append(\"/home/ashutosh/Dropbox/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Initializing Spark and Spark context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from pyspark import SparkContext\n",
    "    from pyspark import SparkConf\n",
    "    from pyspark.sql import SparkSession\n",
    "\n",
    "except ImportError as e:\n",
    "    sys.exit(1)\n",
    "\n",
    "conf = SparkConf().setMaster(\"local[*]\").setAppName(\"My App\")\n",
    "sc = SparkContext(conf=conf)\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Spark calculating the number of times a product is ordered by each user<br>\n",
    "parse1: x[5] is the product_id and x[6] is the user_id<br>\n",
    "parse2: returning it in the form easy to read in pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp = sc.textFile(\"filename.csv\")\n",
    "def parse1(x):\n",
    "    x = str(x)\n",
    "    x = x.split(\",\")\n",
    "    return ((int(x[6]),int(x[5])),1)\n",
    "def parse2(x):\n",
    "    return (x[0][0],x[0][1],x[1])\n",
    "temp = temp.filter(lambda x: \"order_id\" not in x).map(parse1).reduceByKey(lambda x,y: x+y).map(parse2)\n",
    "temp = temp.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Developing a dataframe from the sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label = ['user_id','product_id','count']\n",
    "temp = pd.DataFrame.from_records(temp,columns=label)\n",
    "temp.to_csv(\"filename.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another example of using the Spark calculation\n",
    "\n",
    "In this example, a numpy array of orders is made<br>\n",
    "where the input is +1 when order for a product observed\n",
    "\n",
    "parse1: is used to find which products are ordered in each order<br>\n",
    "parse2: is used to sort the customer orders in their order number (visits)<br>\n",
    "parse3: is used to sum the orders to find out the frequency of each order<br>\n",
    "\n",
    "The it is converted into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "working_data_format = sc.textFile(\"working_data_format.csv\")\n",
    "\n",
    "def parse1(x):\n",
    "    x = x.split(\",\")\n",
    "    y = np.array([0]*len(products))\n",
    "    y[products.index(int(x[5]))] += 1\n",
    "    return ((int(x[6]),int(x[7])),y) # user_id, order_number, y\n",
    "\n",
    "def parse2(x):\n",
    "    return (x[0][1],(x[0][0],x[1].tolist())) # order_number, user_id, y\n",
    "\n",
    "def parse3(x):\n",
    "    return (x[1][0],x[1][1],x[0])\n",
    "\n",
    "working_data = working_data.filter(lambda x: \"order_id\" not in x and int(x.split(\",\")[4]) in products).map(parse1).reduceByKey(lambda  x,y: x+y).map(parse2).sortByKey().map(parse3).reduceByKey(lambda x,y:x+y)\n",
    "\n",
    "working_data_format = working_data.collect()\n",
    "label = ['user_id','input','order_number']\n",
    "working_data_format = pd.DataFrame.from_records(working_data_format,columns=label)\n",
    "working_data_format.to_csv(\"working_data_format.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
